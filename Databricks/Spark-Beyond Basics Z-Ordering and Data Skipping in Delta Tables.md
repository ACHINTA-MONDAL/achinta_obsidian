---
modified: 2024-08-11T01:33:46+05:30
---
If spark could speak, it would say things about Data Engineers that would put them in depression and give them anxiety for a foreseeable future! ğŸ«¢ğŸ˜¬

I mean, These Data Engineers (including me ğŸ‘€) assumes spark can fix the issues which they couldn't (coz they can't codeâ€¦. I am talking about meğŸ™ˆ).

But Spark DOES fix it! One of such issues it fixes is of executing a query 10â€“12 times faster by applying Z-Ordering.

# Z-Ordering

Say you have one delta table which is queried â€œnâ€ number of times in a day. And these queries filter on one specific column every time it runs. ğŸ¤”

Spark already usesÂ [_Adaptive Query Optimization (AQE)_](https://towardsdev.com/spark-beyond-basics-adaptive-query-execution-012a47a4c013)Â to get appropriate number of partitions and to performÂ [_predicate pushdown_](https://towardsdev.com/spark-beyond-basics-adaptive-query-execution-012a47a4c013)Â to fasten up these queries.

But Spark is still not at its peak here! We need to help spark unleash its powersğŸ˜¤! It's known to us that weâ€™ll be applying filter on a specific column, so what we can do is, we can enable Z-Ordering on that column ğŸ¤¨

Enabling Z-Ordering on the column will:

1. _Repartition_Â the table on the specified column
2. _Sort within the partition_Â on the specified column

Let's take an example for better understanding: ğŸ¤“

## Scenario

Consider a delta table with some not-so-interesting columns and â€œNameâ€ (interesting one!) column. We already have 4Â _optimized_Â partitions after Sparkâ€™s AQE.

![](https://miro.medium.com/v2/resize:fit:1264/1*NTjN7IZFjdxioKqVGTI2Sw.png)

querying on the partitioned delta table

- As soon as we run the mentioned query on the table,Â _Spark will look at the min-max values in delta transaction logs and ask_, does this file contain the data I need. ğŸ§
- Hence, spark is going to check the first 3 partitions but won't check the last partition since minÂ _Name is_Â â€œDanâ€ and thus this partition won't include â€œBradâ€.Â **_This is called Data Skipping_**Â ğŸ˜²

> Why will it check first 3 partitions: coz for all three, min Names (Bob, Andy, Brad) is â‰¤ Brad

![](https://miro.medium.com/v2/resize:fit:1318/1*w43IyOph8dBUfg6YhLFCwA.png)

Irregular data skipping

- But this kind ofÂ **_Data Skipping_**Â can be termed asÂ **_irregular_**Â data skipping, since the partitions are notÂ _Optimized_Â for â€˜Nameâ€™ column
- Let's apply Z-Ordering on â€˜Nameâ€™ column and check what happens! âœ¨

![](https://miro.medium.com/v2/resize:fit:1290/1*fO2yKp6JqsZCK7wlROjspQ.png)

repartition and sort within partitions performed on â€˜Nameâ€™ column

![](https://miro.medium.com/v2/resize:fit:638/1*X8xEUM5V0QATJLin_4IbKg.png)

- Now, if you run the above query, Spark will check only the first partition, thus acing theÂ **_Data Skipping._**Â ğŸ˜‰

> Note: The column you choose to Z-Order by should have high cardinality, i.e., a large number of distinct values

## Syntax of Z-Ordering:

To Z-order data, you specify the columns to order on in theÂ `ZORDER BY`Â clause.

> Z-Ordering will be applied by running OPTIMIZE with ZORDER BY clause

OPTIMIZE table_name ZORDER BY column_name

## Benefits of Z-Ordering:

1. Z-ordering can lead to significant speedups, potentially reducing query execution times from minutes to secondsğŸ¤¯
2. Appropriate number of partitions, which creates balance in the cores of the spark worker nodes where tasks are executedâš–ï¸

## Challenges with Z-Ordering:

1. Choosing partitioning columns is a complicated process ğŸ¥´
2. Z-Ordering jobs are expensive and require longer write times â±ï¸
3. Concurrent writes arenâ€™t possible with tables with Z-Ordering enabled

When you weigh the benefits and challenges, you will surely be in dilemmaÂ _to use or not to use._Â ğŸ˜µâ€ğŸ’«

Databricks felt this dilemma too, and they came up with a state-of-the-art solution:Â **_Liquid Clustering_**Â ğŸ¥‚

Iâ€™ll be covering Liquid Clustering in my next blog, so watch out!

If you liked the blog, please clap ğŸ‘ to make this reach to all the Data Engineers.

Thanks for reading! ğŸ˜