---
share_link: https://share.note.sx/1u5t26ny#JaJV3XUzvm0EGDmRqJh2BXofBflslCiCUB2rwz+A63o
source: https://medium.com/@uzzaman.ahmed/pyspark-normal-and-misc-functions-a-comprehensive-guide-fb6e2c61fb77
share_updated: 2024-07-02T01:12:33+05:30
tags:
  - PySpark
---
# Intro

There are a lot of funtions in PySpark, most of them we discussed in my Built-in Functions series. In this article we are going to go over some normal and misc functions that are not mentioned in those articles, but are used widely in the PySpark data transformations.

## 1. COALESCE

`coalesce` : Returns the first non-null value in the input column list.

> You want to create a new column that contains the first non-null value across columns `col1`, `col2`, and `col3`. You can achieve this using the `coalesce()` function as follows

from pyspark.sql.functions import coalesce  
  
`df = spark.createDataFrame([`  
    `(1, 100, None, 200),`  
    `(2, None, 300, None),`  
    `(3, 400, None, None)`  
`], ["id", "col1", "col2", "col3"])`  
  
`df = df.withColumn("first_non_null_value", coalesce("col1", "col2", "col3"))`  
  
`df.show()`  
  
# Output  
+---+----+----+----+--------------------+  
| id|col1|col2|col3|first_non_null_value|  
+---+----+----+----+--------------------+  
|  1| 100|null| 200|                100 |  
|  2|null| 300|null|                300 |  
|  3| 400|null|null|                400 |  
+---+----+----+----+--------------------+

_As you can see, the_ `_coalesce()_` _function takes the columns_ `_col1_`_,_ `_col2_`_, and_ `_col3_` _as arguments and returns the first non-null value across those columns in a new column called_ `_first_non_null_value_`_. If all three columns are null for a row, then the_ `_first_non_null_value_` _column will also be null for that row._

## 2. CRC32

`crc32` : Computes a cyclic redundancy check value for the input column.

`from pyspark.sql.functions import crc32`  
`from pyspark.sql import SparkSession`  
  
`spark = SparkSession.builder.appName("CRC32 Example").getOrCreate()`  
  
`data = [("foo",), ("bar",), ("baz",)]`  
`df = spark.createDataFrame(data, ["string_col"])`  
  
`df = df.withColumn("crc32_hash", crc32(df["string_col"]))`  
  
`df.show()`  
  
# Output  
+----------+----------+  
|string_col|crc32_hash|  
+----------+----------+  
|       foo|  23563736|  
|       bar|   8219656|  
|       baz|  34698363|  
+----------+----------+

_In this example, we first create a PySpark DataFrame with a single string column called_ `_string_col_`_. We then use the_ `_crc32()_` _function to compute the CRC32 hash value of each string in the_ `_string_col_` _column and store the result in a new column called_ `_crc32_hash_`_. As you can see from the output, the_ `_crc32_hash_` _column contains the 32-bit hash value generated by the CRC32 algorithm for each input string._

## 3. ENCODE

`encode()`: Encodes a string column using the specified character set.

`from pyspark.sql.functions import encode`  
`from pyspark.sql import SparkSession`  
  
`spark = SparkSession.builder.appName("Encode Example").getOrCreate()`  
  
`data = [("Hello",), ("World",), ("PySpark",)]`  
`df = spark.createDataFrame(data, ["string_col"])`  
  
`df = df.withColumn("encoded", encode(df["string_col"], "UTF-8"))`  
  
`df.show()`  
  
# Output  
+----------+--------------------+  
|string_col|             encoded|  
+----------+--------------------+  
|     Hello|[48, 65, 6C, 6C, ...|  
|     World|[57, 6F, 72, 6C, ...|  
|   PySpark|[50, 79, 53, 70, ...|  
+----------+--------------------+

_In this example, we use the_ `_encode()_` _function to encode the strings in the_ `_string_col_` _column using the UTF-8 encoding. The result is a new column called_ `_encoded_` _that contains a binary array of bytes for each input string._

## 4. DECODE

`decode()` : Decodes a binary column using the specified character set.

`from pyspark.sql.functions import decode`  
`from pyspark.sql import SparkSession`  
  
`spark = SparkSession.builder.appName("Decode Example").getOrCreate()`  
  
`data = [([48, 65, 6C, 6C, 6F],), ([87, 6F, 72, 6C, 64],), ([80, 79, 53, 70, 61, 72, 6B],)]`  
`df = spark.createDataFrame(data, ["binary_col"])`  
  
`df = df.withColumn("decoded", decode(df["binary_col"], "UTF-8"))`  
  
`df.show()`  
  
# Output  
+--------------------+-------+  
|          binary_col|decoded|  
+--------------------+-------+  
|[48, 65, 6C, 6C, ...|  Hello|  
|[87, 6F, 72, 6C, ...|  World|  
|[80, 79, 53, 70, ...|PySpark|  
+--------------------+-------+

_In this example, we use the_ `_decode()_` _function to decode the binary arrays of bytes in the_ `_binary_col_` _column using the UTF-8 encoding. The result is a new column called_ `_decoded_` _that contains the decoded strings for each input binary array of bytes._

## 5 . GREATEST

`greatest()` : Returns the greatest value from a list of input columns.

> Suppose you have a PySpark DataFrame that contains three numeric columns, and you want to find the maximum value for each row across these columns. You can use the `greatest()` function.

from pyspark.sql.functions import greatest  
from pyspark.sql import SparkSession  
  
spark = SparkSession.builder.appName("Greatest Example").getOrCreate()  
  
data = [(10, 20, 30), (50, 40, 30), (1, 2, 3)]  
df = spark.createDataFrame(data, ["col1", "col2", "col3"])  
  
df = df.withColumn("max_value", greatest(df["col1"], df["col2"], df["col3"]))  
  
df.show()  
  
# Output  
+----+----+----+---------+  
|col1|col2|col3|max_value|  
+----+----+----+---------+  
|  10|  20|  30|       30|  
|  50|  40|  30|       50|  
|   1|   2|   3|        3|  
+----+----+----+---------+

_In this example, we use the_ `_greatest()_` _function to find the maximum value for each row across the three columns_ `_col1_`_,_ `_col2_`_, and_ `_col3_`_. The result is a new column called_ `_max_value_` _that contains the maximum value for each row. As you can see from the output, the_ `_max_value_` _column contains the maximum value across the three columns for each row in the DataFrame._

## 6. LEAST

`least()` : Returns the least value from a list of input columns.

> Suppose you have a PySpark DataFrame that contains three numeric columns, and you want to find the minimum value for each row across these columns. You can use the `_least()_` function

from pyspark.sql.functions import least  
from pyspark.sql import SparkSession  
  
spark = SparkSession.builder.appName("Least Example").getOrCreate()  
  
data = [(10, 20, 30), (50, 40, 30), (1, 2, 3)]  
df = spark.createDataFrame(data, ["col1", "col2", "col3"])  
  
df = df.withColumn("min_value", least(df["col1"], df["col2"], df["col3"]))  
  
df.show()  
  
# Output  
+----+----+----+---------+  
|col1|col2|col3|min_value|  
+----+----+----+---------+  
|  10|  20|  30|       10|  
|  50|  40|  30|       30|  
|   1|   2|   3|        1|  
+----+----+----+---------+

_In this example, we use the_ `_least()_` _function to find the minimum value for each row across the three columns_ `_col1_`_,_ `_col2_`_, and_ `_col3_`_. The result is a new column called_ `_min_value_` _that contains the minimum value for each row. As you can see from the output, the_ `_min_value_` _column contains the minimum value across the three columns for each row in the DataFrame._

## 7. IFNULL

`ifnull()` : Returns the second input if the first input is null, otherwise returns the first input.

> Suppose you have a PySpark DataFrame that contains a column of nullable values, and you want to replace any null values in that column with a default value. You can use the `ifnull()` function.

from pyspark.sql.functions import ifnull  
from pyspark.sql import SparkSession  
  
spark = SparkSession.builder.appName("IfNull Example").getOrCreate()  
  
data = [(1, "foo"), (2, None), (3, "bar")]  
df = spark.createDataFrame(data, ["id", "value"])  
  
df = df.withColumn("value_fixed", ifnull(df["value"], "default_value"))  
  
df.show()  
  
# Output  
+---+------+------------+  
| id| value|value_fixed |  
+---+------+------------+  
|  1|   foo|         foo|  
|  2|  null|default_value|  
|  3|   bar|         bar|  
+---+------+------------+

_In this example, we use the_ `_ifnull()_` _function to replace any null values in the_ `_value_` _column with the default value_ `_"default_value"_`_. The result is a new column called_ `_value_fixed_` _that contains the original values from the_ `_value_` _column, except with null values replaced by the default value. As you can see from the output, the_ `_value_fixed_` _column contains the original values from the_ `_value_` _column where they exist, and the default value where they don't exist._

## 8. NULLIF

`nullif()` : Returns null if the two input values are equal, otherwise returns the first input.

> Suppose you have a PySpark DataFrame that contains a column with values that you want to replace with nulls under certain conditions.

from pyspark.sql.functions import nullif  
from pyspark.sql import SparkSession  
  
spark = SparkSession.builder.appName("NullIf Example").getOrCreate()  
  
data = [("apple", 5), ("orange", 0), ("banana", 10)]  
df = spark.createDataFrame(data, ["fruit", "count"])  
  
df = df.withColumn("count_null", nullif(df["count"], 0))  
  
df.show()  
  
# Output  
+------+-----+----------+  
| fruit|count|count_null|  
+------+-----+----------+  
| apple|    5|         5|  
|orange|    0|      null|  
|banana|   10|        10|  
+------+-----+----------+

_In this example, we use the_ `_nullif()_` _function to replace the value_ `_0_` _in the_ `_count_` _column with null. The result is a new column called_ `_count_null_` _that contains the original values from the_ `_count_` _column, except that the value_ `_0_` _has been replaced by null. As you can see from the output, the_ `_count_null_` _column contains the original values from the_ `_count_` _column where they exist, and null where they don't exist._

## 9. NANVL

`nanvl()` : Returns the second input if the first input is NaN, otherwise returns the first input.

> Suppose you have a PySpark DataFrame that contains two columns with numeric values, and you want to replace any `NaN` (Not a Number) values in one column with the corresponding values from the other column.

from pyspark.sql.functions import nanvl  
from pyspark.sql import SparkSession  
  
spark = SparkSession.builder.appName("Nanvl Example").getOrCreate()  
  
data = [(1.0, float('nan')), (2.0, 3.0), (float('nan'), 4.0)]  
df = spark.createDataFrame(data, ["col1", "col2"])  
  
df = df.withColumn("col1_fixed", nanvl(df["col1"], df["col2"]))  
  
df.show()  
  
# Output  
+----+----+----------+  
|col1|col2|col1_fixed|  
+----+----+----------+  
| 1.0| NaN|       1.0|  
| 2.0| 3.0|       2.0|  
| NaN| 4.0|       4.0|  
+----+----+----------+

_In this example, we use the_ `_nanvl()_` _function to replace any_ `_NaN_` _values in the_ `_col1_` _column with the corresponding values from the_ `_col2_` _column. The result is a new column called_ `_col1_fixed_` _that contains the original values from the_ `_col1_` _column, except that_ `_NaN_` _values have been replaced with the corresponding values from the_ `_col2_` _column. As you can see from the output, the_ `_col1_fixed_` _column contains the original values from the_ `_col1_` _column where they exist, and values from the_ `_col2_` _column where_ `_NaN_` _values exist in_ `_col1_`_._

## 10. RAND

`rand()` : Returns a random number between 0 and 1.

> Suppose you have a PySpark DataFrame with some data, and you want to add a new column with random values between 0 and 1.

from pyspark.sql.functions import rand  
from pyspark.sql import SparkSession  
  
spark = SparkSession.builder.appName("Rand Example").getOrCreate()  
  
data = [("apple", 5), ("orange", 0), ("banana", 10)]  
df = spark.createDataFrame(data, ["fruit", "count"])  
  
df = df.withColumn("rand_val", rand())  
  
df.show()  
  
# Output  
+------+-----+------------------+  
| fruit|count|          rand_val|  
+------+-----+------------------+  
| apple|    5|0.6076930657339104|  
|orange|    0|0.8206940181479154|  
|banana|   10|0.3588242680396245|  
+------+-----+------------------+

_In this example, we use the_ `_rand()_` _function to generate a new column called_ `_rand_val_` _with random values between 0 and 1. As you can see from the output, the_ `_rand_val_` _column contains different random values for each row in the DataFrame._

## 11. RANDN

`randn()` : Returns a random number drawn from a standard normal distribution.

> Suppose you have a PySpark DataFrame with some data, and you want to add a new column with random values from a standard normal distribution (with mean 0 and variance 1)

from pyspark.sql.functions import randn  
from pyspark.sql import SparkSession  
  
spark = SparkSession.builder.appName("Randn Example").getOrCreate()  
  
data = [("apple", 5), ("orange", 0), ("banana", 10)]  
df = spark.createDataFrame(data, ["fruit", "count"])  
  
df = df.withColumn("randn_val", randn())  
  
df.show()  
  
# Output  
+------+-----+--------------------+  
| fruit|count|           randn_val|  
+------+-----+--------------------+  
| apple|    5| -0.0469822900115337|  
|orange|    0|   0.643670497206959|  
|banana|   10|  -1.366247889212852|  
+------+-----+--------------------+

_In this example, we use the_ `_randn()_` _function to generate a new column called_ `_randn_val_` _with random values from a standard normal distribution. As you can see from the output, the_ `_randn_val_` _column contains different random values for each row in the DataFrame._

## 12. SPARK_PARTITION_ID

`spark_partition_id()` : Returns the ID of the current partition.

> Suppose you have a PySpark DataFrame with some data that has been partitioned into multiple partitions, and you want to add a new column that indicates the partition ID for each row.

from pyspark.sql.functions import spark_partition_id  
from pyspark.sql import SparkSession  
  
spark = SparkSession.builder.appName("Spark Partition ID Example").getOrCreate()  
  
data = [("apple", 5), ("orange", 0), ("banana", 10), ("pear", 3), ("kiwi", 8)]  
df = spark.createDataFrame(data, ["fruit", "count"]).repartition(3)  
  
df = df.withColumn("partition_id", spark_partition_id())  
  
df.show()  
  
# Output  
+------+-----+------------+  
| fruit|count|partition_id|  
+------+-----+------------+  
|orange|    0|           1|  
|banana|   10|           2|  
| pear |    3|           0|  
|kiwi  |    8|           2|  
| apple|    5|           1|  
+------+-----+------------+

_In this example, we use the_ `_repartition()_` _function to partition the DataFrame into 3 partitions. We then use the_ `_spark_partition_id()_` _function to generate a new column called_ `_partition_id_` _that indicates the partition ID for each row in the DataFrame. As you can see from the output, the_ `_partition_id_` _column contains different partition IDs for each row in the DataFrame._

## 13. INPUT_FILE_NAME

`input_file_name()` : Returns the file name of the current input file being processed.

> Suppose you have a PySpark DataFrame that is reading data from multiple files in a directory, and you want to add a new column that indicates the input file name for each row.

from pyspark.sql.functions import input_file_name  
from pyspark.sql import SparkSession  
  
spark = SparkSession.builder.appName("Input File Name Example").getOrCreate()  
  
df = spark.read.text("/path/to/directory/*")  
  
df = df.withColumn("input_file_name", input_file_name())  
  
df.show()  
  
# Output  
+--------------------+---------------------+  
|               value|      input_file_name|  
+--------------------+---------------------+  
|This is the first...|/path/to/director... |  
|This is the secon...|/path/to/director... |  
|This is the third...|/path/to/director... |  
|This is the fourt...|/path/to/director... |  
+--------------------+---------------------+

_In this example, we use the_ `_read.text()_` _function to read data from all files in the specified directory. We then use the_ `_input_file_name()_` _function to generate a new column called_ `_input_file_name_` _that indicates the input file name for each row in the DataFrame. As you can see from the output, the_ `_input_file_name_` _column contains the input file name for each row in the DataFrame._

## 14. RAISE

`raise()` : Throws an exception with the provided error message.

> In this example, we use a try-except block to catch any errors that may occur in the code. We intentionally cause an error by filtering the DataFrame with a condition that will filter out all rows, resulting in an empty DataFrame. We then catch the error and use the `raise` statement to raise a new error with a custom message.

`from pyspark.sql import SparkSession`  
  
`spark = SparkSession.builder.appName("Raise Error Example").getOrCreate()`  
  
`try:`  
    `# some code that may raise an error`  
    `data = [("apple", 5), ("orange", 0), ("banana", 10), ("pear", 3), ("kiwi", 8)]`  
    `df = spark.createDataFrame(data, ["fruit", "count"])`  
    `df = df.filter("count < 0")  # this will filter out all rows, causing an error`  
    `df.show()`  
      
`except Exception as e:`  
    `# handle the error by raising a new error with a custom message`  
    `raise ValueError("An error occurred: {}".format(str(e)))`

## Output

Traceback (most recent call last):  
  File "/path/to/script.py", line 10, in <module>  
    df.show()  
  File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/dataframe.py", line 440, in show  
    print(self._jdf.showString(n, 20, vertical))  
  File "/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__  
    answer, self.gateway_client, self.target_id, self.name)  
  File "/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py", line 63, in deco  
    return f(*a, **kw)  
  File "/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value  
    format(target_id, ".", name))  
py4j.protocol.Py4JJavaError: An error occurred while calling o57.showString.  
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost, executor driver): java.lang.RuntimeException: java.lang.RuntimeException: The value of the boolean expression is not true.  
 at org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1$$anonfun$apply$3.apply(BatchEvalPythonExec.scala:129)  
 at org.apache.spark.sql.execution.python.BatchEvalPythonExec$$anonfun$doExecute$1$$anonfun$apply$3.apply(BatchEvalPythonExec.scala:128)  
 at scala.collection.Iterator$$anon$11.next(Iterator.scala:410)  
...

_In this example, the code intentionally raises an error by filtering out all rows from the DataFrame. The_ `_raise_` _statement is then used to raise a new error with a custom message. The output shows the error message and the traceback, which includes the custom error message we raised._

# Conclusion

The above article explains a few normal and misc functions in PySpark and how they can be used with examples. This is a part of PySpark functions [series](https://medium.com/@uzzaman.ahmed/list/pyspark-builtin-functions-67bec40e6c5c) by me, check out my PySpark [SQL 101](https://medium.com/@uzzaman.ahmed/list/pyspark-sql-basics101-d80bd574842d) series and other [articles](https://medium.com/@uzzaman.ahmed/list/data-comparison-series-bb0f05ff3f71). Enjoy Reading..

Apache Spark Functions Guide — [https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)?


-----------------------------------------------------------------------------------------

Hi everyone! I started posting contents about Spark interview questions for SWE/Data Engineers, mainly for Spark Optimization related questions. I aim to continuously write about ten posts about Spark optimization. After the series of these posts, you will ace technical interviews related to Spark! Although this post aims for helping technical interview rounds, any Spark users will find this series insightful and help your learning!

> _“Disclaimer: The views and opinions expressed in this blog post are solely my own and do not reflect those of any entity with which I have been, am now, or will be affiliated. This content was written during a period in which the author was not affiliated with nor belong to any organization that could influence my perspectives. As such, these are my personal insights, shared without any external bias or influence.”_

#What is Partitioning ?

Partitioning in Spark refers to the process of dividing a large dataset into smaller, manageable parts (called partitions) that can be processed in parallel across different nodes of a Spark cluster. This is essential for distributed computing, as it allows Spark to perform operations on datasets in a more efficient and scalable manner.

# Why is Partitioning Important?

1. Parallelism: By partitioning data, Spark can leverage the full power of the cluster to process data in parallel, significantly speeding up data processing tasks.
2. Reduced Data Shuffling: Proper partitioning can minimize the amount of data that needs to be shuffled across the network during wide transformations (e.g., groupBy, join). Data shuffling is a very expensive operation in terms of time and network I/O, and reducing it can greatly improve performance.
3. Resource Optimization: Efficient partitioning ensures that the workload is evenly distributed across the cluster, preventing certain nodes from becoming bottlenecks while others remain underutilized.

# Example

You can use the `partition` method to change the number of partitions for an existing RDD. Proper partitioning is essential for optimizing PySpark’s performance, especially when dealing with large datasets and operations that involve data shuffling, such as joins and groupBy operations. The choice of an appropriate number of partitions and partitioning strategy depends on your specific use case and data distribution.

Ex)

from pyspark import SparkConf, SparkContext

conf = SparkConf().setAppName("PartitionExample")  
sc = SparkContext(conf=conf)# Create an RDD with custom partitions  
data = list(range(1, 101))  
num_partitions = 4  # Specify the number of partitions  
rdd = sc.parallelize(data, num_partitions)

## Dynamic Allocation

Note that if you enable Spark’s dynamic allocation, it can automatically adjust the number of partitions based on the workload. This can be a helpful option for optimizing resource usage.

`from pyspark import SparkConf`  
`from pyspark.sql import SparkSession`

`# Create a SparkConf and enable dynamic allocation`  
`spark_conf = SparkConf().setAppName("DynamicAllocationExample") \`  
                       `.set("spark.dynamicAllocation.enabled", "true")# Create a SparkSession with the configured SparkConf`  
`spark = SparkSession.builder.config(conf=spark_conf).getOrCreate()# You can also set other dynamic allocation-related configurations, such as min and max executors`  
`spark.conf.set("spark.dynamicAllocation.minExecutors", "1")`  
`spark.conf.set("spark.dynamicAllocation.maxExecutors", "4")`  
`spark.conf.set("spark.shuffle.service.enabled", "true")  # Enables external shuffle service`

In this example, we create a `SparkConf` and enable dynamic allocation by setting `"spark.dynamicAllocation.enabled"` to `"true"`. We create a `SparkSession` using the configured `SparkConf` . You can also set other dynamic allocation-related configurations:

- `spark.dynamicAllocation.minExecutors`: The minimum number of executors to allocate.
- `spark.dynamicAllocation.maxExecutors`: The maximum number of executors to allocate.
- `spark.shuffle.service.enabled`: Enables the external shuffle service, which can improve dynamic allocation's performance.

## Typical Partition problem and solutions

Apache Spark, by default, sets the number of shuffle partitions to 200. Depending on your data size and the resources of your cluster, this number might not be optimal.

- If you have too many partitions, you might end up with many small tasks, increasing scheduling overhead.
- If you have too few partitions, you might end up with a few long-running tasks and less parallelism.

For the many small partition problem, having too many partitions in a Spark application can indeed lead to a high scheduling overhead, as each partition corresponds to a task in Spark. When there are many small tasks, the time spent on task scheduling and execution overhead can outweigh the actual computation time, leading to inefficient processing.

To fix this issue, you can coalesce or repartition your RDD or DataFrame to reduce the number of partitions. The choice between `coalesce` and `repartition` depends on your specific needs:

- `coalesce` method is used to decrease the number of partitions in the DataFrame. It can efficiently minimize the partitions without shuffling the data across the partitions because it tries to combine partitions on the same executor. This method is useful when you want to reduce the number of partitions and minimize data movement.
- `repartition` method, on the other hand, redistributes the data across the specified number of partitions, potentially causing a full data shuffle. This method can be used when you want to increase or decrease the number of partitions and are willing to incur the cost of a full shuffle for a more uniform distribution of data.

from pyspark.sql import SparkSession  
  
# Initialize a SparkSession  
spark = SparkSession.builder.appName("OptimizePartitions").getOrCreate()  
  
# Example DataFrame  
df = spark.read.csv("path/to/your/data.csv", header=True)  
  
# Assume df has a large number of partitions, we want to reduce it  
num_partitions = 10  # Target number of partitions  
df_coalesced = df.coalesce(num_partitions)  
  
# You can now perform further transformations/actions on df_coalesced  
df_coalesced.show()

The above example is to use coalesce() where you simply decrease the number of partitions in the DataFrame, and the below approach is repartition() where you want to shuffle data based on the partition key.

# Continuing from the previous example  
# If you need to either increase or decrease the number of partitions and shuffle data  
num_partitions = 5  # New target number of partitions  
df_repartitioned = df.repartition(num_partitions)  
  
# Use df_repartitioned for further data processing  
df_repartitioned.show()

Let’s deep dive coalesce() vs. repartition() more details.

## Coalesce and Repartition

Both `coalesce` and `repartition` operations are used to control the number of partitions and their distribution. Coalesce can be used to reduce the number of partitions, while repartition allows you to increase or change the partitioning scheme.

## What is Coalesce?

1. Definition: `coalesce` is a Spark method used to reduce the number of partitions in a DataFrame or RDD. It combines existing partitions to lower the total count, primarily used to optimize for data locality and reduce shuffle operations.[]
2. Shuffle Avoidance: Unlike other methods, `coalesce` minimizes data movement as it does not shuffle all data but rather merges existing partitions.

**So, when to Use Coalesce ? There are two cases we can think of:**

- Post-Filtering Scenario: Particularly useful after filtering a large dataset, where many partitions may end up being sparsely populated.
- Minimal Shuffle Requirement: Ideal when reducing partition count without the overhead of a full shuffle is desired.

## What is Repartition?

1. Definition: `repartition` in Spark is used to increase or decrease the number of partitions in an RDD or DataFrame. It involves shuffling data across the cluster, unlike `coalesce`.
2. Full Shuffle: Repartitioning redistributes data across new partitions, which can help in cases of data skew but at the cost of a full shuffle.

So, when to use repartition? There are **two cases we can think of:**

- Increasing Partitions: Essential when dealing with skewed data or when the goal is to increase parallelism in processing.
- Data Balancing: Useful for evenly distributing data across the cluster, especially when initial partitioning leads to uneven data distribution.

Example: Suppose you have a large dataset of customer transactions, and you find that a significant portion of transactions belongs to a handful of customers. This can lead to some partitions being much larger than others.

There are a few options to do.

Option1: Repartitioning based on a column (or multiple) that ensures better distribution such as date.

# Assuming df is your DataFrame  
  
repartitioned_df = df.repartition("day")

Option2: Repartition with a Specific Number of Partitions

In this example, we increase the number of partitions to 100, regardless of the current partition count. This can be useful if the initial number of partitions is too low for the dataset’s size, leading to large and unbalanced partitions.

# Increasing the number of partitions  
repartitioned_df = df.repartition(100)

## Deciding Between Coalesce and Repartition

1. Data Size and Skewness: Consider the size and distribution of your data. Use `coalesce` for reducing partitions on evenly distributed data, and `repartition` for handling skewed data or increasing parallelism.
2. Stage of Processing: Post-aggregation or filtering stages are more suited for `coalesce`, while initial stages or stages requiring balanced data distribution can benefit from `repartition`.

FYI: How does Spark determines partition key by default?

For DataFrames and Datasets, partition keys are determined based on the columns used in operations that involve shuffling, such as `groupBy`, `join`, or `repartition`. When you perform these operations, Spark uses the values of the specified columns as partition keys to distribute the data across partitions.

- groupBy and join: The columns specified in these operations act as the partition keys. Spark groups or joins the data based on the values of these columns, effectively determining the distribution of data across the cluster.

That’s it for today, thanks reading this far! I hope you find this series useful.

Scenario: You work at a large e-commerce company that has a huge amount of customer transaction data in Parquet format. The dataset is so large that you’re having trouble processing it efficiently with your PySpark jobs. After analyzing the problem, you realize that the number of partitions for the dataset is not optimal for your use case, and you need to either repartition or coalesce the data.

Repartition: You decide to use the repartition method to increase the number of partitions for the dataset. You want to increase the number of partitions from 100 to 200, to allow for more parallelism and better resource utilization. Here’s an example code snippet:

pythonCopy code

from pyspark.sql.functions import spark_partition_id

# Load the data from Parquet  
df = spark.read.parquet("s3://my-bucket/customer-transactions")# Get the current number of partitions  
num_partitions_before = df.rdd.getNumPartitions()# Repartition the data  
df = df.repartition(200)# Get the new number of partitions  
num_partitions_after = df.rdd.getNumPartitions()# Print the before and after number of partitions  
print("Number of partitions before repartition: {}".format(num_partitions_before))  
print("Number of partitions after repartition: {}".format(num_partitions_after))

Output:

Number of partitions before repartition: 100  
Number of partitions after repartition: 200

Coalesce: Alternatively, you could use the coalesce method to decrease the number of partitions for the dataset. For example, you want to decrease the number of partitions from 100 to 50, to reduce the overhead of managing a large number of partitions. Here’s an example code snippet:

from pyspark.sql.functions import spark_partition_id

# Load the data from Parquet  
df = spark.read.parquet("s3://my-bucket/customer-transactions")# Get the current number of partitions  
num_partitions_before = df.rdd.getNumPartitions()# Coalesce the data  
df = df.coalesce(50)# Get the new number of partitions  
num_partitions_after = df.rdd.getNumPartitions()# Print the before and after number of partitions  
print("Number of partitions before coalesce: {}".format(num_partitions_before))  
print("Number of partitions after coalesce: {}".format(num_partitions_after))

Output:

Number of partitions before coalesce: 100  
Number of partitions after coalesce: 50

As you can see, the repartition method increases the number of partitions, while the coalesce method decreases the number of partitions. It’s important to note that coalesce is more efficient than repartition because it does not perform a full shuffle of the data. However, it can only be used to decrease the number of partitions, while repartition can be used to increase or decrease the number of partitions.

-----------------------------------------------------------------------------------------
When dealing with large datasets in PySpark, understanding how to manage data distribution is crucial. Two commonly used methods for this purpose are `repartition()` and `partitionBy()`. While they might seem similar at first glance, they serve different purposes and are used in different contexts. This article will delve into the differences between these two methods, provide guidance on when to use each, and explore advanced strategies for controlling the number of output files when writing data.

# Understanding Repartition

The `repartition()` function in PySpark is used to increase or decrease the number of partitions in a DataFrame. When you call `repartition()`, Spark shuffles the data across the network to create new partitions.

The primary use case for `repartition()` is to optimize the parallel processing capabilities of Spark. If the data is not well-distributed across partitions, or if the number of partitions is not well-suited to the cluster's configuration, it can lead to inefficient processing. For instance, having too few partitions can lead to underutilization of resources, as there might not be enough tasks to distribute across all cores. On the other hand, having too many partitions can also be problematic, as each task comes with some scheduling overhead.

`repartition()` can be particularly useful when you have skewed data and want to distribute it more evenly across your partitions. By redistributing the data, `repartition()` can help ensure that the workload is more evenly distributed across the tasks, leading to more efficient processing.

For a more in depth understanding of partitions, see my article on partition shuffling:

[

## PySpark: A Guide to Partition Shuffling

### Boost your Spark performance by employing effective shuffle partition strategies

medium.com



](https://medium.com/@tomoscorbin/boosting-efficiency-in-pyspark-a-guide-to-partition-shuffling-9a5af77703ea?source=post_page-----cfde90aa3622--------------------------------)

# Understanding PartitionBy

The `partitionBy()` function in PySpark is used when writing a DataFrame out to a file system. It determines how the output files will be organized in the file system based on the columns you specify.

When you use `partitionBy()`, PySpark creates a directory structure based on the specified columns, and the data is partitioned on the disk accordingly. This can be particularly useful when you're dealing with large datasets and want to read only a subset of data based on certain column values. By partitioning the data on disk, read operations can be more efficient as only the relevant partitions need to be read.

For example, if you have a DataFrame with sales data and you’re frequently running queries for specific regions, you might choose to partition your data by ‘region’ when writing it out. This way, when you run a query for sales in a specific region, Spark can quickly locate and read only the relevant partition, rather than having to scan through the entire dataset. Let’s say you often run queries for sales in the US. By partitioning your data on ‘region’, Spark can directly access the ‘US’ partition when you next run a query for US sales. This means Spark can bypass the need to process the entire dataset, significantly reducing the time and computational resources required to return your query results.

# When to Use Repartition vs PartitionBy

Now that we understand what `repartition()` and `partitionBy()` do, let's discuss when to use each.

Use `repartition()` when:

1. You want to increase or decrease the number of partitions in your DataFrame.
2. You are dealing with skewed data and want to distribute it more evenly across your partitions.
3. You are performing operations like `join` or `groupby` that can cause data shuffling. Increasing the number of partitions can help improve performance.

Use `partitionBy()` when:

1. You are writing a DataFrame out to a file system and want to organize the output files based on certain column values.
2. You are dealing with large datasets and want to read only a subset of data based on certain column values. Partitioning the data on disk can make these read operations more efficient.

# The Unusual Behaviour of PartitionBy

While `partitionBy()` is a powerful tool for organizing your data on disk, it's important to understand its behavior to avoid unexpected results. One key aspect to note is how `partitionBy()` interacts with the partitions of your DataFrame.

When you call `df.write.partitionBy('column')`, each of the original partitions in `df` is written independently. That is, each of your original partitions is sub-partitioned separately based on the 'column', and a separate file is written for each sub-partition. This means that the number of output files depends on the distribution of data in the original partitions.

Let’s illustrate this with an example. Suppose you have a DataFrame and you want to write it to disk and partition it by ‘DayOfWeek’. Since there are 7 distinct values in this column (one for every day of the week), you expect Spark to write 7 partitions/output files. However, this is not necessarily the case. In fact, if your initial DataFrame was made up of 10 partitions, you would end up with 70 output files. This is because each of the 10 partitions is sub-partitioned into 7 ‘dayOfWeek’ partitions, resulting in 10 * 7 = 70 output files.

On the other hand, if each of your original 10 partitions contains data from exactly one day, you would end up with only 10 output files. This is because each partition is sub-partitioned into just one ‘dayOfWeek’ partition, resulting in the same number of output files as original partitions.

This behavior can be surprising and might not align with your initial expectations. You would expect `partitionBy()` to create a global partitioning based on the specified column, resulting in a number of output files equal to the number of unique values in the column. However, `partitionBy()` operates on the level of individual partitions, leading to a potentially larger number of output files.

This can have significant implications for performance and storage. Having a large number of small files can be inefficient for both storage and query performance, as each file carries some metadata overhead and opening each file can take time. Therefore, understanding this behavior is crucial when using `partitionBy()`, as it can influence the organization and size of your output files.

In the next section, we'll explore some advanced strategies to control the number of output files when using `partitionBy()`.

# Advanced Strategies for Controlling Output Files

While `partitionBy()` are useful for managing data distribution, it might not always give you the desired control over the number of output files when writing data. Here are a couple of advanced strategies to control for this:

## **Repartitioning Before Writing**

One strategy to control the number of output files is to use `repartition()` before `partitionBy()`. This allows you to control the number of partitions in memory before writing out the data. Here's how you can do it:

df.repartition(7, "DayOfWeek").write.partitionBy("DayOfWeek").parquet("path")

In this example, the DataFrame is first repartitioned into 7 partitions based on ‘DayOfWeek’. `repartition()` uses a hash-based partitioner, which ensures that the unique ‘DayOfWeek’ values make their way into each partition. Then, when writing out the data with `partitionBy()`, each of these 7 partitions is written independently, resulting in a maximum of 7 output files for each unique value in 'DayOfWeek'.

Although the hash-based partitioner ensures that all rows with the same key (in this case, the same ‘DayOfWeek’ value) end up in the same partition, in some cases, different keys can end up in the same partition (this is known as a hash collision). While each partition will contain all the rows for a subset of the unique ‘DayOfWeek’ values, it is possible that multiple unique ‘DayOfWeek’ values (i.e., different days of the week) end up in the same partition.

Hash collisions are generally not a problem in terms of data integrity. The hash function used in repartitioning ensures that all rows with the same key (in this case, the same ‘DayOfWeek’ value) will always end up in the same partition. So, even if two different keys (i.e., two different ‘DayOfWeek’ values) hash to the same partition, the rows themselves are still distinguishable based on their keys. However, hash collisions can mean that you don’t get exactly the number of output files you were expecting. Hash collisions are relatively rare, but it’s a possibility to be aware of, especially when dealing with large datasets and a large number of keys.

It’s important to note that while using `repartition()` before `partitionBy()` can help control the number of output files, it can also lead to uneven partition sizes and task execution times due to the nature of hash-based partitioning. If many keys hash to the same partition, that partition could end up with more data than others, leading to an imbalance known as data skew. This skew can impact the performance of your Spark jobs, as tasks processing larger partitions will take longer to complete, potentially slowing down your overall job. For instance, if some ‘DayOfWeek’ values are associated with many more rows than others (e.g., Monday might have more rows due to higher activity, whereas Sunday might have fewer rows), this can lead to lopsided partition sizes. This method therefore requires careful consideration of your data distribution to avoid potential performance issues.

## **Sorting Before Writing**

Another strategy to control the number of output files is to sort your data before writing it out. This approach globally sorts your data and then finds splits that break up the data into evenly-sized partitions. Here’s how you can do it:

df.sort("DayOfWeek").write.parquet("path")

In this example, the DataFrame is first sorted based on ‘DayOfWeek’. This global sorting ensures that the data is organized in a specific order before it is written out. When the data is written, Spark finds splits that break up the data into evenly-sized partitions. This results in output files that are nearly equal in size.

This approach can be particularly useful when you want to control the size of your output files. By sorting the data before writing, you can ensure that each output file contains a roughly equal amount of data. This can be beneficial for downstream tasks that read these files, as they can expect a consistent amount of data from each file.

However, there’s a caveat to this approach. If your use-case requires all rows with the same key to be in the same partition, this approach might not be suitable. This is because the global sorting can result in rows with the same key being spread across multiple partitions. For instance, if you’re sorting by ‘DayOfWeek’, rows for the same day might end up in different partitions if that day’s data spans across the splits found by Spark.

Therefore, while sorting before writing can help control the size and number of output files, it requires careful consideration of your data distribution and use-case requirements.

# Best Practices

1. **Choosing the Right Number of Partitions**: As a rule of thumb, the number of partitions should be larger than the number of cores available in your Spark cluster to ensure that all cores have data to process. However, having too many partitions can also be problematic, as each task comes with some scheduling overhead. A common practice is to have 2–3 tasks per CPU core in your cluster.
2. **Handling Skewed Data**: If your data is skewed, meaning that some keys have significantly more values than others, this can lead to some tasks taking much longer to complete than others. In such cases, you might consider using techniques like salting (adding a random value to the key) to distribute the data more evenly across partitions.
3. **Minimising Shuffles**: Shuffling data across the network is one of the most expensive operations in Spark. Whenever possible, try to minimise shuffles by repartitioning your data in a way that aligns with your subsequent transformations. For example, if you’re going to join two DataFrames on a certain key, it might be beneficial to repartition both DataFrames on that key beforehand. Read more about shuffling [here](https://medium.com/@tomoscorbin/boosting-efficiency-in-pyspark-a-guide-to-partition-shuffling-9a5af77703ea).
4. **Optimising for Sequential Reads**: When writing out your data, consider how it will be read in subsequent tasks. Sequential reads are faster than random reads, so if you’re going to be scanning large portions of your data, it might be beneficial to partition your data in a way that aligns with the order in which you’ll be reading it. For instance, if you have a DataFrame of sales data and you frequently run queries that filter by ‘region’ and then by ‘date’, it might be beneficial to partition your data by ‘region’ and then sort within each partition by ‘date’. This way, when you run your query, Spark can quickly locate the ‘region’ partition and then read the ‘date’ data sequentially, which is faster than having to jump around different parts of the partition to find the relevant ‘date’ data.
5. **Monitoring and Adjusting**: The optimal partitioning strategy can depend on many factors, including the size and distribution of your data, the configuration of your Spark cluster, and the specific transformations you’re performing. Therefore, it’s important to monitor the performance of your Spark jobs and adjust your partitioning strategy as needed. The Spark UI provides useful information about task duration, data shuffling, and other metrics that can help you diagnose performance issues and fine-tune your partitioning strategy.

# Conclusion

In conclusion, while `repartition()` and `partitionBy()` might seem similar, they serve different purposes. `repartition()` is about how the data is distributed across partitions in memory during computation, while `partitionBy()` is about how the data is partitioned on disk when writing out to a file system. Understanding these differences and when to use each can help you manage your data more effectively and efficiently in PySpark. Furthermore, advanced strategies like sorting before writing or using a custom partitioner can give you more control over the number of output files when writing data. Always test different strategies to find the one that works best for your use case.